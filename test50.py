import networkx as nx
import random
import time
import numpy as np
import matplotlib.pyplot as plt

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torchvision import datasets, transforms
from torch.utils.data import DataLoader, Subset, random_split, Dataset

# é…ç½®å‚æ•°
NUM_CLIENTS = 10
SELECT_CLIENTS = 5
NUM_EPOCHS = 2
NUM_ROUNDS = 100
BATCH_SIZE = 128
Recent_TX = 3
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
TOP_K_MODELS = 5
N_RECENT_MODELS = 11
DIRICHLET_ALPHA = 0.01
SPARSIFICATION_RATIO = 0.3

# å…¨å±€èŠ‚ç‚¹å‚æ•°
GLOBAL_NODE_ID = -2
GLOBAL_AGGREGATION_INTERVAL = 10
N_REFERENCE_CANDIDATES = 6

# CIFAR10æ¨¡å‹å®šä¹‰ï¼ˆä¿®æ”¹ç‚¹1ï¼‰
class CIFAR10CNN(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)
        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, padding=1)
        self.pool = nn.MaxPool2d(2, 2)
        self.fc1 = nn.Linear(64 * 4 * 4, 512)
        self.fc2 = nn.Linear(512, 10)
        self.dropout = nn.Dropout(0.2)

    def forward(self, x):
        x = self.pool(F.relu(self.conv1(x)))
        x = self.pool(F.relu(self.conv2(x)))
        x = self.pool(F.relu(self.conv3(x)))
        x = x.view(-1, 64 * 4 * 4)
        x = F.relu(self.fc1(x))
        x = self.dropout(x)
        x = self.fc2(x)
        return x

class FilteredDataset(Dataset):
    def __init__(self, dataset, target_digits):
        self.dataset = dataset
        self.target_digits = target_digits
        self.indices = [i for i, (_, label) in enumerate(dataset) if label in target_digits]

    def __len__(self):
        return len(self.indices)
    
    def __getitem__(self, idx):
        return self.dataset[self.indices[idx]]

# æ•°æ®åŠ è½½ä¿®æ”¹ä¸ºCIFAR10ï¼ˆä¿®æ”¹ç‚¹2ï¼‰
def load_data_dirichlet():
    transform_train = transforms.Compose([
        transforms.RandomCrop(32, padding=4),
        transforms.RandomHorizontalFlip(),
        transforms.ToTensor(),
        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))
    ])
    
    transform_test = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))
    ])
    
    train_set = datasets.CIFAR10('./data', train=True, download=True, transform=transform_train)
    test_set = datasets.CIFAR10('./data', train=False, transform=transform_test)
    
    node_primary_digits = {i: [i%10, (i+1)%10] for i in range(NUM_CLIENTS)}
    
    digit_indices = {d: [] for d in range(10)}
    for idx, (_, label) in enumerate(train_set):
        digit_indices[label].append(idx)
    
    node_to_indices = {i: [] for i in range(NUM_CLIENTS)}
    for digit in range(10):
        indices = digit_indices[digit]
        dirichlet_params = np.ones(NUM_CLIENTS) * DIRICHLET_ALPHA
        for node_id, digits in node_primary_digits.items():
            if digit in digits:
                dirichlet_params[node_id] = 10.0
        
        proportions = np.random.dirichlet(dirichlet_params)
        proportions = (np.cumsum(proportions) * len(indices)).astype(int)
        start_idx = 0
        for node_id in range(NUM_CLIENTS):
            end_idx = proportions[node_id] if node_id != NUM_CLIENTS-1 else len(indices)
            node_to_indices[node_id].extend(indices[start_idx:end_idx])
            start_idx = end_idx
    
    client_datasets = [Subset(train_set, indices) for indices in node_to_indices.values()]
    client_test_datasets = [FilteredDataset(test_set, node_primary_digits[i]) for i in range(NUM_CLIENTS)]
    
    train_loaders = [DataLoader(ds, batch_size=BATCH_SIZE, shuffle=True) for ds in client_datasets]
    client_test_loaders = [DataLoader(ds, batch_size=512) for ds in client_test_datasets]
    test_loader = DataLoader(test_set, batch_size=512)
    
    # æ‰“å°æ•°æ®åˆ†å¸ƒ
    for i, ds in enumerate(client_datasets):
        labels = [train_set[idx][1] for idx in ds.indices]
        unique, counts = np.unique(labels, return_counts=True)
        distribution = dict(zip(unique, counts))
        primary_digits = node_primary_digits[i]
        primary_count = sum(counts[np.where(unique == d)[0][0]] if d in unique else 0 for d in primary_digits)
        total_count = sum(counts)
        print(f"Client {i}: Primary digits {primary_digits} ratio {primary_count/total_count:.2f}")
    
    return train_loaders, client_test_loaders, test_loader, node_primary_digits

# ä¿®æ”¹å®¢æˆ·ç«¯è®­ç»ƒå‡½æ•°ï¼ˆä¿®æ”¹ç‚¹3ï¼‰
def client_update(model, loader, optimizer):
    model.train()
    model.to(DEVICE)
    for _ in range(NUM_EPOCHS):
        for X, y in loader:
            X, y = X.to(DEVICE), y.to(DEVICE)
            optimizer.zero_grad()
            output = model(X)  # ä¿®æ”¹è¾“å…¥å¤„ç†
            loss = F.cross_entropy(output, y)
            loss.backward()
            optimizer.step()
    return model.cpu().state_dict()

# å…¶ä½™å·¥å…·å‡½æ•°éœ€è¦ä¿®æ”¹è¾“å…¥å¤„ç†éƒ¨åˆ†
def get_model_importance_score(params):
    total_score = 0
    for key, value in params.items():
        if 'weight' in key or 'bias' in key:
            total_score += torch.sum(torch.abs(value)).item()
    return total_score

def topk_sparsification_fedavg(params_list, k=TOP_K_MODELS):
    if len(params_list) <= k:
        return federated_average(params_list)
    
    importance_scores = [get_model_importance_score(params) for params in params_list]
    topk_indices = np.argsort(importance_scores)[-k:]
    topk_params = [params_list[i] for i in topk_indices]
    return federated_average(topk_params)

def federated_average(params_list):
    avg_params = {}
    for key in params_list[0].keys():
        avg_params[key] = torch.stack([p[key].float() for p in params_list], dim=0).mean(dim=0)
    return avg_params

def evaluate(model, test_loader):
    model.to(DEVICE)
    model.eval()
    correct = 0
    total = 0
    with torch.no_grad():
        for X, y in test_loader:
            X, y = X.to(DEVICE), y.to(DEVICE)
            output = model(X)  # ä¿®æ”¹è¾“å…¥å¤„ç†
            pred = output.argmax(dim=1)
            correct += pred.eq(y).sum().item()
            total += len(y)
    return 100. * correct / total if total > 0 else 0









# DAGåŒºå—é“¾ç±»éœ€è¦ä¿®æ”¹æ¨¡å‹åˆå§‹åŒ–ï¼ˆä¿®æ”¹ç‚¹4ï¼‰
class DAGBlockchainFL:
    def __init__(self, num_nodes=10, node_primary_digits=None):
        self.num_nodes = num_nodes
        self.graph = nx.DiGraph()
        self.node_positions = {}
        self.current_id = 0
        self.tx_params = []
        self.tx_accuracies = []
        self.tx_global_accuracies = []
        self.tx_importance_scores = []
        self.node_primary_digits = node_primary_digits
        self.round = 0
        self._init_genesis()
        self.last_global_tx = 0
        self.latest_tx_per_node = {i: 0 for i in range(num_nodes)}
        
        self.global_acc_history = []
        self.round_history = []
        self.local_acc_history = []
        self.round_transactions = {}
        self.nodes_active_in_round = {}

    def _init_genesis(self):
        initial_model = CIFAR10CNN()  # ä¿®æ”¹ä¸ºCIFAR10æ¨¡å‹
        self.graph.add_node(self.current_id, node_id=-1)
        self.node_positions[self.current_id] = (0, 0)
        self.tx_params.append(initial_model.state_dict())
        self.tx_accuracies.append(0.0)
        self.tx_global_accuracies.append(0.0)
        self.tx_importance_scores.append(0.0)
        self.current_id += 1

    def evaluate_model_accuracy(self, model_params, node_id=None, use_global=False):
        model = CIFAR10CNN()  # ä¿®æ”¹ä¸ºCIFAR10æ¨¡å‹
        model.load_state_dict(model_params)
        
        if use_global or node_id is None:
            return evaluate(model, test_loader)
        elif node_id < len(client_test_loaders):
            return evaluate(model, client_test_loaders[node_id])
        else:
            return evaluate(model, test_loader)
    
    def evaluate_model_on_primary_digits(self, model_params, node_id):
        if node_id >= len(client_test_loaders):
            return 0.0
            
        model = CIFAR10CNN()  # ä¿®æ”¹ä¸ºCIFAR10æ¨¡å‹
        model.load_state_dict(model_params)
        return evaluate(model, client_test_loaders[node_id])

    def topk_sparsification_aggregation(self, create_global_tx=False):
        """ä½¿ç”¨ä¸¤ä¸ªå…¨å±€èŠ‚ç‚¹ä¹‹é—´çš„å…¨éƒ¨äº¤æ˜“è¿›è¡Œè”é‚¦å¹³å‡ï¼Œæ¯ä¸ªèŠ‚ç‚¹åªä¿ç•™æœ€æ–°äº¤æ˜“"""
        if len(self.graph) <= 1:  # åªæœ‰åˆ›ä¸–å—
            print("No transactions to aggregate!")
            return 0.0, None, None

        # è·å–æ‰€æœ‰æ™®é€šäº¤æ˜“ï¼ˆæ’é™¤åˆ›ä¸–å—å’Œä¹‹å‰çš„å…¨å±€èŠ‚ç‚¹ï¼‰
        all_txs = [n for n in self.graph.nodes 
                if n != 0 and self.graph.nodes[n].get('node_id') != GLOBAL_NODE_ID]
        
        # è·å–åœ¨ä¸Šæ¬¡å…¨å±€äº¤æ˜“ä¹‹åäº§ç”Ÿçš„äº¤æ˜“
        start_tx = self.last_global_tx + 1
        candidate_txs = [tx for tx in all_txs if tx >= start_tx]
        
        # å¦‚æœèŒƒå›´ä¸ºç©ºåˆ™ä½¿ç”¨æœ€è¿‘çš„äº¤æ˜“
        if not candidate_txs:
            candidate_txs = sorted(all_txs)[-1:]  # ä½¿ç”¨æœ€è¿‘1ä¸ªäº¤æ˜“

        print(f"\nğŸ“Š Found {len(candidate_txs)} candidate transactions between TX{start_tx}-{max(candidate_txs)}")

        # æŒ‰èŠ‚ç‚¹åˆ†ç»„ï¼Œä¿ç•™æ¯ä¸ªèŠ‚ç‚¹æœ€æ–°çš„äº¤æ˜“
        node_tx_dict = {}
        for tx in candidate_txs:
            node_id = self.graph.nodes[tx].get('node_id', -1)
            # æ’é™¤åˆ›ä¸–å—å’Œå…¨å±€èŠ‚ç‚¹ï¼ˆç†è®ºä¸Šä¸ä¼šå­˜åœ¨ï¼‰
            if node_id >= 0:  
                if node_id not in node_tx_dict or tx > node_tx_dict[node_id]:
                    node_tx_dict[node_id] = tx

        # è·å–æœ€ç»ˆäº¤æ˜“åˆ—è¡¨
        selected_txs = list(node_tx_dict.values())
        print(f"ğŸ” After deduplication: {len(selected_txs)} transactions from {len(node_tx_dict)} nodes")
        print(f"   Node distribution: {node_tx_dict}")

                # ==== æ–°å¢éƒ¨åˆ†ï¼šæŒ‰èŠ‚ç‚¹åˆ†ç±»åˆ†ç»„å¹¶é€‰æ‹©Top-K ====
        from collections import defaultdict

        # åˆ›å»ºåˆ†ç±»åˆ°äº¤æ˜“çš„æ˜ å°„ï¼ˆåˆ†ç±»ç”±ä¸»è¦æ•°å­—å†³å®šï¼‰
        category_to_txs = defaultdict(list)
        for tx in selected_txs:
            node_id = self.graph.nodes[tx].get('node_id', -1)
            if node_id == -1:
                continue  # æ’é™¤æ— æ•ˆèŠ‚ç‚¹
            
            # è·å–è¯¥èŠ‚ç‚¹çš„ä¸»è¦å…³æ³¨æ•°å­—ä½œä¸ºåˆ†ç±»é”®
            primary_digits = self.node_primary_digits.get(node_id, [])
            category = tuple(sorted(primary_digits))  # æ’åºä»¥ç¡®ä¿ä¸åŒé¡ºåºè§†ä¸ºç›¸åŒåˆ†ç±»
            category_to_txs[category].append(tx)

        # å¯¹æ¯ä¸ªåˆ†ç±»é€‰æ‹©é‡è¦æ€§Top-Kçš„äº¤æ˜“
        final_selected_txs = []
        for category, txs in category_to_txs.items():
            # è·å–è¿™äº›äº¤æ˜“çš„é‡è¦æ€§åˆ†æ•°
            txs_with_scores = []
            for tx in txs:
                # ç¡®ä¿é‡è¦æ€§åˆ†æ•°å·²è®¡ç®—
                if tx >= len(self.tx_importance_scores) or self.tx_importance_scores[tx] == 0.0:
                    score = get_model_importance_score(self.tx_params[tx])
                    self.tx_importance_scores[tx] = score
                else:
                    score = self.tx_importance_scores[tx]
                txs_with_scores.append((tx, score))
            
            # æŒ‰åˆ†æ•°é™åºæ’åº
            sorted_txs = sorted(txs_with_scores, key=lambda x: x[1], reverse=True)
            
            # é€‰æ‹©Top-Kï¼ˆä½¿ç”¨å…¨å±€TOP_K_MODELSå‚æ•°ï¼‰
            k = min(TOP_K_MODELS, len(sorted_txs))
            selected = [tx for tx, _ in sorted_txs[:k]]
            final_selected_txs.extend(selected)
            print(f"  Category {category}: Selected {k} transactions (max score {sorted_txs[0][1]:.1f})")

        # ==== ç»“æŸæ–°å¢éƒ¨åˆ† ====


        # è·å–å¯¹åº”å‚æ•°å¹¶è”é‚¦å¹³å‡
        params_list = [self.tx_params[tx] for tx in selected_txs]
        global_params = federated_average(params_list)

        # è¯„ä¼°å…¨å±€æ¨¡å‹
        


        # åˆ›å»ºå…¨å±€äº¤æ˜“
        global_tx_id = None
        if create_global_tx:
            global_model = CIFAR10CNN()
            global_model.load_state_dict(global_params)
            acc = evaluate(global_model, test_loader)
            global_tx_id = self.current_id
            self.graph.add_node(global_tx_id, node_id=GLOBAL_NODE_ID)
            self.node_positions[global_tx_id] = (global_tx_id * 2.2, -1)
            self.graph.add_edges_from([(p, global_tx_id) for p in selected_txs])

            self.tx_params.append(global_params)
            self.tx_accuracies.append(0.0)
            self.tx_global_accuracies.append(acc)
            self.tx_importance_scores.append(get_model_importance_score(global_params))
            
            self.current_id += 1
            self.last_global_tx = global_tx_id
            print(f"\nğŸŒ Created Global Transaction {global_tx_id} with {len(selected_txs)} refs")
            print(f"   global accuracy: {acc:.2f}%")

            return acc, global_params, global_tx_id

    def find_best_models_for_node(self, node_id, recent_n=N_REFERENCE_CANDIDATES):
        """ä¸ºæŒ‡å®šèŠ‚ç‚¹æ‰¾åˆ°åœ¨å…¶ä¸»è¦æ•°å­—ä¸Šè¡¨ç°æœ€å¥½çš„æ¨¡å‹
        
        Args:
            node_id: èŠ‚ç‚¹ID
            recent_n: è€ƒè™‘çš„æœ€è¿‘äº¤æ˜“æ•°é‡
            
        Returns:
            list: æŒ‰æ€§èƒ½æ’åºçš„äº¤æ˜“IDåˆ—è¡¨ï¼Œæœ€å¥½çš„åœ¨å‰
        """
        # è·å–é™¤åˆ›ä¸–å—å¤–çš„æ‰€æœ‰äº¤æ˜“
        all_txs = [n for n in self.graph.nodes if n != 0]
        if not all_txs:
            return [0]  # å¦‚æœåªæœ‰åˆ›ä¸–å—ï¼Œåˆ™è¿”å›åˆ›ä¸–å—
            
        # è€ƒè™‘æœ€è¿‘çš„Nä¸ªäº¤æ˜“
        recent_n = min(recent_n, len(all_txs))
        recent_txs = sorted(all_txs)[-recent_n:]

        if self.last_global_tx > 0 and self.last_global_tx not in recent_txs:
            recent_txs.append(self.last_global_tx)
        
        # è¯„ä¼°è¿™äº›äº¤æ˜“åœ¨è¯¥èŠ‚ç‚¹ä¸»è¦æ•°å­—ä¸Šçš„è¡¨ç°
        performance = []
        for tx in recent_txs:
            # ç¡®ä¿å‚æ•°å­˜åœ¨
            if tx < len(self.tx_params):
                # è¯„ä¼°æ¨¡å‹åœ¨è¯¥èŠ‚ç‚¹ä¸»è¦æ•°å­—ä¸Šçš„å‡†ç¡®ç‡
                acc = self.evaluate_model_on_primary_digits(self.tx_params[tx], node_id)
                performance.append((tx, acc))
        
        # æŒ‰å‡†ç¡®ç‡é™åºæ’åº
        sorted_performance = sorted(performance, key=lambda x: x[1], reverse=True)
        
        # è¿”å›æ’åºåçš„äº¤æ˜“IDåˆ—è¡¨
        return [tx for tx, _ in sorted_performance]

    def generate_transaction(self, current_round=None):
        """ç”Ÿæˆæ–°äº¤æ˜“çš„æ ¸å¿ƒæ–¹æ³•ï¼ˆåŸºäºä¸»è¦æ•°å­—è¡¨ç°çš„å¼•ç”¨ç‰ˆï¼‰"""
        if not self.graph.nodes:
            return []

        current_nodes = list(self.graph.nodes)
        selected_nodes = []
        initial_model_id = 0  # åˆå§‹æ¨¡å‹äº¤æ˜“IDå›ºå®šä¸º0
        new_tx_count = 0  # æ–°äº§ç”Ÿçš„äº¤æ˜“è®¡æ•°
        
        # åˆå§‹åŒ–å½“å‰è½®æ¬¡çš„äº¤æ˜“åˆ—è¡¨
        if current_round is not None and current_round not in self.round_transactions:
            self.round_transactions[current_round] = []
            self.nodes_active_in_round[current_round] = set()

        # ç¬¬ä¸€æ­¥ï¼šé€‰æ‹©éœ€è¦ç”Ÿæˆäº¤æ˜“çš„èŠ‚ç‚¹
        for node_id in range(self.num_nodes):
            if random.random() < SELECT_CLIENTS/NUM_CLIENTS:
                # æ‰¾åˆ°åœ¨è¯¥èŠ‚ç‚¹ä¸»è¦æ•°å­—ä¸Šè¡¨ç°æœ€å¥½çš„Recent_TXä¸ªæ¨¡å‹
                # ä¿®æ”¹åçš„ä»£ç 
                if self.last_global_tx > 0 and current_round %3 == 0:
                    # å­˜åœ¨æœ€æ–°çš„å…¨å±€äº¤æ˜“ï¼Œå¼ºåˆ¶å¼•ç”¨
                    parents = [self.last_global_tx]
                else:
                    # åŸæœ‰é€»è¾‘ï¼šåŸºäºä¸»è¦æ•°å­—é€‰æ‹©æœ€ä½³æ¨¡å‹
                    best_models = self.find_best_models_for_node(node_id, N_REFERENCE_CANDIDATES)
                    num_parents = min(Recent_TX, len(best_models))
                    parents = best_models[:num_parents]
                    # ç¡®ä¿çˆ¶èŠ‚ç‚¹ä¸ä¸ºç©º
                    if not parents and initial_model_id in current_nodes:
                        parents = [initial_model_id]
            
                # å¦‚æœçˆ¶èŠ‚ç‚¹åˆ—è¡¨ä¸ºç©ºä¸”å­˜åœ¨åˆ›ä¸–å—ï¼Œåˆ™å¼•ç”¨åˆ›ä¸–å—
                if not parents and initial_model_id in current_nodes:
                    parents = [initial_model_id]

                if parents:
                    selected_nodes.append((node_id, parents))

        # ç¬¬äºŒæ­¥ï¼šå¤„ç†æ¯ä¸ªè¢«é€‰ä¸­çš„èŠ‚ç‚¹
        new_tx_ids = []
        for node_id, parents in selected_nodes:
            # è·å–çˆ¶äº¤æ˜“çš„æ¨¡å‹å‚æ•°
            parent_params = []
            for p in parents:
                if p < len(self.tx_params):  # ç¡®ä¿å‚æ•°å­˜åœ¨
                    parent_params.append(self.tx_params[p])

            new_tx_id = self.current_id
            self.latest_tx_per_node[node_id] = new_tx_id  # æ›´æ–°æœ€æ–°äº¤æ˜“

            # è”é‚¦å¹³å‡é€»è¾‘
            if parent_params:
                aggregated_params = federated_average(parent_params)
            else:
                aggregated_params = self.tx_params[0]

            # åˆå§‹åŒ–æœ¬åœ°æ¨¡å‹
            local_model = CIFAR10CNN()
            local_model.load_state_dict(aggregated_params)
            optimizer = optim.SGD(local_model.parameters(), lr=0.01, momentum=0.9)
            updated_params = client_update(
                model=local_model,
                loader=train_loaders[node_id],
                optimizer=optimizer
            )
            
            # è®¡ç®—å‚æ•°é‡è¦æ€§åˆ†æ•°
            importance_score = get_model_importance_score(updated_params)

            # åˆ›å»ºæ–°äº¤æ˜“
            new_tx_id = self.current_id
            self.graph.add_node(new_tx_id, node_id=node_id)  # å…³é”®ï¼è®°å½•ç”ŸæˆèŠ‚ç‚¹
            self.node_positions[new_tx_id] = (
                new_tx_id * 2.2,
                node_id + random.uniform(-0.3, 0.3)
            )
            self.graph.add_edges_from([(p, new_tx_id) for p in parents])
            self.tx_params.append(updated_params)
            self.tx_importance_scores.append(importance_score)
            
            # è¯„ä¼°å¹¶è®°å½•æ–°æ¨¡å‹çš„å‡†ç¡®ç‡
            print(f"\nâœ… tx {new_tx_id}")
            print(f"   node: N{node_id}")
            print(f"   ref: {parents}")
            print(f"   importance score: {importance_score:.2f}")
            
            # æ‰“å°è¯¥èŠ‚ç‚¹ä¸»è¦å…³æ³¨çš„æ•°å­—
            primary_digits = self.node_primary_digits[node_id]
            print(f"   primary digits: {primary_digits}")
            
            # 1. ä½¿ç”¨èŠ‚ç‚¹ä¸“ç”¨æµ‹è¯•é›†è¯„ä¼°ï¼ˆåªåŒ…å«å…¶ä¸»è¦æ•°å­—ï¼‰
            print("   acc on primary digits:", end=" ")
            local_model.load_state_dict(updated_params)
            primary_acc = self.evaluate_model_accuracy(updated_params, node_id)
            self.tx_accuracies.append(primary_acc)
            
            # 2. ä½¿ç”¨å…¨å±€æµ‹è¯•é›†è¯„ä¼°ï¼ˆæ‰€æœ‰10ä¸ªæ•°å­—ï¼‰

            self.current_id += 1
            new_tx_ids.append(new_tx_id)
            new_tx_count += 1
            
            # è®°å½•å½“å‰è½®æ¬¡çš„äº¤æ˜“
            if current_round is not None:
                self.round_transactions[current_round].append((new_tx_id, node_id, primary_acc))
                self.nodes_active_in_round[current_round].add(node_id)
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦æ‰§è¡Œå…¨å±€èšåˆï¼ˆæ¯äº§ç”ŸGLOBAL_AGGREGATION_INTERVALä¸ªæ–°äº¤æ˜“ï¼‰
        if new_tx_count > 0 and current_round %3 == 2:
            print(f"\nğŸŒ Triggering Global Aggregation after {new_tx_count} new transactions")
            self.topk_sparsification_aggregation(create_global_tx=True)
            print(f"Round {current_round} global accuracy: {self.tx_global_accuracies}%")
        
        return new_tx_ids

    
    def calculate_local_average_accuracy(self):
        """è®¡ç®—æ‰€æœ‰èŠ‚ç‚¹çš„æœ€æ–°æ¨¡å‹å¹³å‡å‡†ç¡®åº¦"""
        total_acc = 0.0
        for node_id in range(self.num_nodes):
            tx_id = self.latest_tx_per_node[node_id]
            if tx_id >= len(self.tx_params):
                tx_id = 0  # ä½¿ç”¨åˆ›ä¸–å—å‚æ•°
            model_params = self.tx_params[tx_id]
            acc = self.evaluate_model_accuracy(model_params, node_id=node_id)
            total_acc += acc
        return total_acc / self.num_nodes

    def run_simulation(self, rounds=NUM_ROUNDS):
        """è¿è¡Œæ¨¡æ‹ŸæŒ‡å®šè½®æ•°"""
        print(f"\nğŸš€ Starting simulation for {rounds} rounds...")

        
        for r in range(rounds):
            current_round = r + 1
            print(f"\n===== Round {current_round}/{rounds} =====")
            
            # æ¯è½®ç”Ÿæˆå¤šä¸ªäº¤æ˜“
            for _ in range(1):  # æ¯è½®ç”Ÿæˆäº¤æ˜“
                self.generate_transaction(current_round=current_round)
            
            # è®¡ç®—å½“å‰è½®æ¬¡çš„æœ¬åœ°å¹³å‡ç²¾åº¦
            local_avg_acc = self.calculate_local_average_accuracy()
            self.local_acc_history.append(local_avg_acc)
            
            # åœ¨æ¯è½®ç»“æŸæ—¶æ‰§è¡Œå…¨å±€èšåˆè¯„ä¼°ï¼ˆä¸åˆ›å»ºå…¨å±€äº¤æ˜“ï¼‰
            print(f"\nğŸ“ˆ Evaluating global model at round {current_round}")
            self.topk_sparsification_aggregation(create_global_tx=False)
            
            print(f"Round {current_round} local average accuracy: {local_avg_acc:.2f}%")
        
        return self.round_history, self.global_acc_history, self.local_acc_history
    
    def plot_accuracy(self):
        """ç»˜åˆ¶å…¨å±€æ¨¡å‹ç²¾åº¦å’Œæœ¬åœ°å¹³å‡ç²¾åº¦éšæ—¶é—´å˜åŒ–çš„æ›²çº¿"""
        plt.figure(figsize=(12, 7))

        print("global_acc_history",self.tx_global_accuracies)
        print("local_acc_history",self.local_acc_history)
        
        # ç»˜åˆ¶å…¨å±€ç²¾åº¦æ›²çº¿
        plt.plot(self.round_history, self.global_acc_history, marker='o', 
                 linestyle='-', linewidth=2, markersize=5, label='Global Model Accuracy')
        
        # ç»˜åˆ¶æœ¬åœ°å¹³å‡ç²¾åº¦æ›²çº¿
        plt.plot(self.round_history, self.local_acc_history, marker='s', 
                 linestyle='--', linewidth=2, markersize=5, color='orange', 
                 label='Local Average Accuracy')
        
        plt.title('Global vs Local Model Accuracy Over Rounds', fontsize=16)
        plt.xlabel('Round', fontsize=14)
        plt.ylabel('Accuracy (%)', fontsize=14)
        plt.grid(True, alpha=0.3)
        plt.ylim(50, 100)  # è°ƒæ•´Yè½´èŒƒå›´æ›´å¥½åœ°å±•ç¤ºå‡†ç¡®ç‡å˜åŒ–
        
        # æ·»åŠ å…¨å±€æ¨¡å‹å¹³å‡å€¼å’Œæœ€å¤§å€¼çº¿
        global_avg_acc = np.mean(self.global_acc_history)
        global_max_acc = np.max(self.global_acc_history)
        plt.axhline(y=global_avg_acc, color='blue', linestyle=':', alpha=0.7,
                   label=f'Global Avg: {global_avg_acc:.2f}%')
        plt.axhline(y=global_max_acc, color='g', linestyle='--', 
                    label=f'Maximum: {global_max_acc:.2f}%')
        
        plt.legend()
        plt.tight_layout()
        plt.savefig('global_accuracy.png', dpi=300)
        plt.show()

        print(self.global_acc_history)
        print(self.local_acc_history)

# åˆå§‹åŒ–ç³»ç»Ÿå’Œæ•°æ®
train_loaders, client_test_loaders, test_loader, node_primary_digits = load_data_dirichlet()
global_model = CIFAR10CNN().to(DEVICE)  # åˆå§‹åŒ–å…¨å±€æ¨¡å‹
print("Initial model accuracy:", evaluate(global_model, test_loader))

# åˆ›å»ºå¹¶è¿è¡Œæ¨¡æ‹Ÿ
sim = DAGBlockchainFL(num_nodes=NUM_CLIENTS, node_primary_digits=node_primary_digits)
sim.run_simulation(rounds=NUM_ROUNDS)
sim.plot_accuracy()

